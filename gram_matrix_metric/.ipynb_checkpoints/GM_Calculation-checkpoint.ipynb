{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GMdistance_update\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import libraries and packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "import librosa\n",
    "from IPython.display import Audio, display\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as stats\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "from torch.nn.modules.module import _addindent\n",
    "\n",
    "import copy\n",
    "import os\n",
    "import math\n",
    "\n",
    "from scipy.stats import ortho_group\n",
    "\n",
    "\n",
    "import soundfile as sf\n",
    "from matplotlib.pyplot import figure\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics.pairwise import cosine_distances as cos\n",
    "from sklearn.metrics import mean_squared_error as mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_scale(img):\n",
    "    img = np.log1p(img)\n",
    "    return img\n",
    "\n",
    "def inv_log(img):\n",
    "    img = np.exp(img) - 1.\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "SYNTHETIC_LENGTH_FACTOR = 1 \n",
    "\n",
    "num_steps=1\n",
    "\n",
    "####################################################################################################\n",
    "#Gram matrix dot product multiplies: RFMSTACK*(feature_size/RFMSALEFACTOR)^2\n",
    "RANDOM_PROJECTION= \"None\" # \"None\", \"Gaussian\", \"Sparse\"   (n=None means no random projection at all)\n",
    "RFMSCALEFACTOR=16   #RM project to size relative to feature dimension\n",
    "RFMSTACK=16         # how many different RMs to stack\n",
    "####################################################################################################\n",
    "\n",
    "numStreams=6\n",
    "learning_Rate= 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "outname = 'rftests' \n",
    "runs = 1 \n",
    "\n",
    "N_FFT = 512 \n",
    "K_HOP = 128 \n",
    "N_FREQ=257\n",
    "\n",
    "\"\"\"use a custom convolutional network randomly initialized\"\"\" \n",
    "use01scale = False \n",
    "boundopt = False \n",
    "whichChannel = \"freq\"\n",
    "N_FILTERS = 512\n",
    "\n",
    "possible_kernels = [2,4,8,16,64,128,256,512,1024,2048]\n",
    "hor_filters = [0]*numStreams\n",
    "for j in range(numStreams):\n",
    "    hor_filters[j]=possible_kernels[j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_spectrogram=log_scale\n",
    "inv_log_spectrogram=inv_log\n",
    "stft_channels = N_FFT \n",
    "hop_size =  K_HOP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU available = False\n"
     ]
    }
   ],
   "source": [
    "use_cuda = torch.cuda.is_available() #use GPU if available\n",
    "print('GPU available =',use_cuda)\n",
    "dtype = torch.cuda.FloatTensor if use_cuda else torch.FloatTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_audio_spectum(filename, tifresi=True):\n",
    "    x, fs  = sf.read(filename)\n",
    "    N_SAMPLES = len(x)   \n",
    "    R = np.abs(librosa.stft(x, n_fft=N_FFT, hop_length=K_HOP, win_length=N_FFT,  center=False))    \n",
    "    return R, fs\n",
    "\n",
    "\n",
    "def findMinMax(img):\n",
    "    return int(math.floor(np.amin(img))),int(math.ceil(np.amax(img)))\n",
    "\n",
    "def img_scale(img,datasetMin,datasetMax,scaleMin,scaleMax):\n",
    "    \"\"\"scales input numpy array from [datasetMin,datasetMax] -> [scaleMin,scaleMax]\"\"\"    \n",
    "    shift = (scaleMax-scaleMin) / (datasetMax-datasetMin)\n",
    "    scaled_values = shift * (img-datasetMin) + scaleMin\n",
    "    return scaled_values\n",
    "\n",
    "def img_invscale(img,datasetMin,datasetMax,scaleMin,scaleMax):\n",
    "    \"\"\"scales input numpy array from [scaleMin,scaleMax] -> [datasetMin,datasetMax]\"\"\"\n",
    "    shift = (datasetMax-datasetMin) / (scaleMax-scaleMin)\n",
    "    scaled_values = shift * (img-scaleMin) + datasetMin\n",
    "    return scaled_values\n",
    "    \n",
    "\n",
    "    def db_scale(img,scale=80):\n",
    "        img = librosa.amplitude_to_db(img)\n",
    "        shift = float(np.amax(img))\n",
    "        img = img - shift \n",
    "        img = img/scale \n",
    "        img = img + 1\n",
    "        img = np.maximum(img, 0)\n",
    "        return img, shift\n",
    "\n",
    "    def inv_db(img,shift,scale=80):\n",
    "        img = img - 1 \n",
    "        img = img * scale \n",
    "        img = img + shift\n",
    "        img = librosa.db_to_amplitude(img)    \n",
    "        return img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the function to prepare the input. The two inputs will be pre-processed thorugh the same function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new function to prepare and inspect the inputs. \n",
    "def prepare_input(FILENAME):\n",
    "    R, fs = read_audio_spectum(FILENAME, tifresi)\n",
    "    if tifresi :\n",
    "        a_style = log_spectrogram(R)\n",
    "    else :\n",
    "        a_style = log_scale(R)\n",
    "    if use01scale == True:\n",
    "        a_min,a_max = findMinMax(a_style)\n",
    "        a_style = img_scale(a_style,a_min,a_max,0,1)\n",
    "    \n",
    "    if 0 : \n",
    "        \n",
    "        SPECTOFFSET=0 \n",
    "\n",
    "        lspect=a_style\n",
    "        if use01scale == True:\n",
    "            out_spec = inv_log_spectrogram(lspect+ SPECTOFFSET) \n",
    "        else:\n",
    "            out_spec =inv_log_spectrogram(lspect+SPECTOFFSET) \n",
    "        x = stft_system.invert_spectrogram(out_spec) \n",
    "        display(Audio(x, rate=16000, autoplay=True))\n",
    "\n",
    "    temp_a_style=a_style \n",
    "    N_SAMPLES = a_style.shape[1] \n",
    "    N_FREQ = a_style.shape[0]\n",
    "    \n",
    "    a_style = np.ascontiguousarray(a_style[None,None,:,:])\n",
    "    if whichChannel == \"2d\":\n",
    "        a_style = torch.from_numpy(a_style)\n",
    "    elif whichChannel == \"freq\":\n",
    "        a_style = torch.from_numpy(a_style).permute(0,2,1,3) \n",
    "    elif whichChannel == \"time\":\n",
    "        a_style = torch.from_numpy(a_style).permute(0,3,1,2) \n",
    "\n",
    "    converted_img = Variable(a_style).type(dtype)\n",
    "    return converted_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"Here we create the custom network\"\n",
    "import collections as c\n",
    "\n",
    "if whichChannel == \"2d\":\n",
    "    IN_CHANNELS = 1\n",
    "elif whichChannel == \"freq\":\n",
    "    IN_CHANNELS = N_FREQ\n",
    "elif whichChannel == \"time\":\n",
    "    IN_CHANNELS = N_SAMPLES\n",
    "def weights_init(m,hor_filter):\n",
    "    std = np.sqrt(2) * np.sqrt(2.0 / ((N_FREQ + N_FILTERS) * hor_filter))\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        m.weight.data.normal_(0.0, std)\n",
    "\n",
    "class style_net(nn.Module):\n",
    "    \"\"\"Here create the network you want to use by adding/removing layers in nn.Sequential\"\"\"\n",
    "    def __init__(self,hor_filter):\n",
    "        super(style_net, self).__init__()\n",
    "        self.layers = nn.Sequential(c.OrderedDict([\n",
    "                            ('conv1',nn.Conv2d(IN_CHANNELS,N_FILTERS,kernel_size=(1,hor_filter),bias=False)),\n",
    "                            ('relu1',nn.ReLU())]))\n",
    "\n",
    "            \n",
    "    def forward(self,input):\n",
    "        out = self.layers(input)\n",
    "        return out\n",
    "    \n",
    "\n",
    "cnnlist=[] \n",
    "for j in range(numStreams) :\n",
    "    cnn = style_net(hor_filters[j])\n",
    "    cnn.apply(lambda x, f=hor_filters[j]: weights_init(x,f))\n",
    "    for param in cnn.parameters():\n",
    "        param.requires_grad = False\n",
    "    if use_cuda:\n",
    "        cnn = cnn.cuda()\n",
    "    \n",
    "    cnnlist.append(cnn)\n",
    "\n",
    "\n",
    "content_layers_default = [] \n",
    "style_layers_default = ['relu_1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "RFM={}\n",
    "\n",
    "if RANDOM_PROJECTION==\"Sparse\" : \n",
    "    RFM[1024]=torch.from_numpy(np.random.choice(np.sqrt(3)*np.array([1., 0., -1.]), (1024,int(1024/RFMSCALEFACTOR)), p=[1./6.,2./3., 1./6.])).type(dtype)\n",
    "    RFM[512]=torch.from_numpy(np.random.choice(np.sqrt(3)*np.array([1., 0., -1.]), (512,int(512/RFMSCALEFACTOR)), p=[1./6.,2./3., 1./6.])).type(dtype)\n",
    "    RFM[256]=torch.from_numpy(np.random.choice(np.sqrt(3)*np.array([1., 0., -1.]), (256,int(256/RFMSCALEFACTOR)), p=[1./6.,2./3., 1./6.])).type(dtype)\n",
    "    RFM[128]=torch.from_numpy(np.random.choice(np.sqrt(3)*np.array([1., 0., -1.]), (128,int(128/RFMSCALEFACTOR)), p=[1./6.,2./3., 1./6.])).type(dtype)\n",
    "\n",
    "if RANDOM_PROJECTION==\"Gaussian\" : \n",
    "    RFM[1024]=(torch.from_numpy(ortho_group.rvs(1024)[:int(1024/RFMSCALEFACTOR)]).type(dtype)).unsqueeze(0)\n",
    "    RFM[512]= (torch.from_numpy(ortho_group.rvs(512)[: int(512/RFMSCALEFACTOR)]).type(dtype)).unsqueeze(0)\n",
    "    RFM[256]= (torch.from_numpy(ortho_group.rvs(256)[: int(256/RFMSCALEFACTOR)]).type(dtype)).unsqueeze(0)\n",
    "    RFM[128]= (torch.from_numpy(ortho_group.rvs(128)[: int(128/RFMSCALEFACTOR)]).type(dtype)).unsqueeze(0)\n",
    "    for rm in range(1, RFMSTACK) : \n",
    "        print(\"yes\")\n",
    "        RFM[1024]= torch.cat( (RFM[1024], (torch.from_numpy(ortho_group.rvs(1024)[:int(1024/RFMSCALEFACTOR)]).type(dtype)).unsqueeze(0)), dim=0)\n",
    "        RFM[512]=torch.cat( (RFM[512],    (torch.from_numpy(ortho_group.rvs(512)[ :int(512/RFMSCALEFACTOR)]).type(dtype)).unsqueeze(0)), dim=0)\n",
    "        RFM[256]=torch.cat( (RFM[256],    (torch.from_numpy(ortho_group.rvs(256)[ :int(256/RFMSCALEFACTOR)]).type(dtype)).unsqueeze(0)), dim=0)\n",
    "        RFM[128]=torch.cat( (RFM[128],    (torch.from_numpy(ortho_group.rvs(128)[ :int(128/RFMSCALEFACTOR)]).type(dtype)).unsqueeze(0)), dim=0)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GramMatrix(nn.Module):\n",
    "\n",
    "    def forward(self, input):\n",
    "        a, b, c, d = input.size()         \n",
    "        features = input.view(b, a * c * d)\n",
    "        \n",
    "      \n",
    "        if RANDOM_PROJECTION==\"None\":\n",
    "            features2=features.unsqueeze(0)\n",
    "        else :\n",
    "            features2=torch.matmul(RFM[b], features)\n",
    "        G = torch.matmul(features2, torch.transpose(features2, 1,2))\n",
    "        return G.div(a * c * d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gram(cnn,result,style_img, content_img=None,\n",
    "                               style_weight=1, content_weight=0,\n",
    "                               content_layers=content_layers_default,\n",
    "                               style_layers=style_layers_default, style_img2=None):\n",
    "    cnn = copy.deepcopy(cnn)\n",
    "    \n",
    "    model = nn.Sequential()\n",
    "    layer_list = list(cnn.layers)\n",
    "    \n",
    "    gram = GramMatrix()\n",
    "     \n",
    "    i = 1  \n",
    "    for layer in layer_list:\n",
    "        \n",
    "        if isinstance(layer, nn.Conv2d): \n",
    "            name = \"conv_\" + str(i)\n",
    "            model.add_module(name, layer) \n",
    "\n",
    "            if name in style_layers: \n",
    "                target_feature = model(style_img).clone()\n",
    "                target_feature_gram = gram(target_feature)\n",
    "                target_feature_gram=torch.flatten(target_feature_gram)\n",
    "                target_feature_gram=target_feature_gram.numpy()\n",
    "                target_feature_gram=target_feature_gram.reshape(512,512)\n",
    "                target_feature_gram= StandardScaler().fit_transform(target_feature_gram)\n",
    "                result.append((target_feature_gram))\n",
    "\n",
    "        if isinstance(layer, nn.ReLU):\n",
    "            name = \"relu_\" + str(i)\n",
    "            model.add_module(name, layer)\n",
    "                    \n",
    "            if name in style_layers:\n",
    "                target_feature = model(style_img).clone()\n",
    "                target_feature_gram = gram(target_feature)\n",
    "                target_feature_gram=torch.flatten(target_feature_gram)\n",
    "                target_feature_gram=target_feature_gram.numpy()\n",
    "                target_feature_gram=target_feature_gram.reshape(512,512)\n",
    "                target_feature_gram= StandardScaler().fit_transform(target_feature_gram)\n",
    "                result.append((target_feature_gram))\n",
    "               \n",
    "            i += 1\n",
    "\n",
    "        if isinstance(layer, nn.MaxPool2d): \n",
    "            name = \"pool_\" + str(i)\n",
    "            model.add_module(name, layer)\n",
    "            \n",
    "            if name in style_layers:\n",
    "                target_feature = model(style_img).clone()\n",
    "                target_feature_gram = gram(target_feature)\n",
    "                target_feature_gram=torch.flatten(target_feature_gram)\n",
    "                target_feature_gram=target_feature_gram.numpy()\n",
    "                target_feature_gram=target_feature_gram.reshape(512,512)\n",
    "                target_feature_gram= StandardScaler().fit_transform(target_feature_gram)\n",
    "                result.append((target_feature_gram))\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gram_v2(cnn,result,style_img, content_img=None,\n",
    "                               style_weight=1, content_weight=0,\n",
    "                               content_layers=content_layers_default,\n",
    "                               style_layers=style_layers_default, style_img2=None):\n",
    "    cnn = copy.deepcopy(cnn)\n",
    "    \n",
    "    model = nn.Sequential()\n",
    "    layer_list = list(cnn.layers)\n",
    "    \n",
    "    gram = GramMatrix()  \n",
    "    i = 1  \n",
    "    for layer in layer_list:\n",
    "        \n",
    "        if isinstance(layer, nn.Conv2d):\n",
    "            name = \"conv_\" + str(i)\n",
    "            model.add_module(name, layer) \n",
    "\n",
    "            if name in style_layers:\n",
    "                target_feature = model(style_img).clone()\n",
    "                target_feature_gram = gram(target_feature)\n",
    "                target_feature_gram=torch.flatten(target_feature_gram)\n",
    "                result.append((target_feature_gram))\n",
    "\n",
    "        if isinstance(layer, nn.ReLU):\n",
    "            name = \"relu_\" + str(i)\n",
    "            model.add_module(name, layer)\n",
    "                    \n",
    "            if name in style_layers:\n",
    "                target_feature = model(style_img).clone()\n",
    "                target_feature_gram = gram(target_feature)\n",
    "                target_feature_gram=torch.flatten(target_feature_gram)\n",
    "                result.append((target_feature_gram))\n",
    "               \n",
    "            i += 1\n",
    "\n",
    "        if isinstance(layer, nn.MaxPool2d): #do the same for maxpool\n",
    "            name = \"pool_\" + str(i)\n",
    "            model.add_module(name, layer)\n",
    "            \n",
    "            if name in style_layers:\n",
    "                target_feature = model(style_img).clone()\n",
    "                target_feature_gram = gram(target_feature)\n",
    "                target_feature_gram=torch.flatten(target_feature_gram)\n",
    "                result.append((target_feature_gram))\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating Euclidean Distance between Spectrogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "131.06195\n",
      "125.40677\n",
      "88.78871\n",
      "3.8292646\n",
      "83.828\n",
      "123.383194\n",
      "131.08781\n",
      "124.83561\n",
      "86.552\n"
     ]
    }
   ],
   "source": [
    "testfolder = 'inputs/new_texture/FM_cf' #change the name and directory of the inout file\n",
    "import os\n",
    "\n",
    "input_imgs = []\n",
    "loss=[]\n",
    "listdir = os.listdir(testfolder)\n",
    "\n",
    "for filename in listdir:\n",
    "    if 'exp-00.00'in filename:\n",
    "        if '.wav'in filename:\n",
    "            input_imgs.append(testfolder+'/'+filename)\n",
    "    \n",
    "\n",
    "input_imgs.sort()\n",
    "#print(\"input_imgs:\",input_imgs)\n",
    "\n",
    "for i in range(len(input_imgs)):\n",
    "    style_img=prepare_input(input_imgs[9])\n",
    "    input_img=prepare_input(input_imgs[i])\n",
    "    df = np.asarray(style_img-input_img)\n",
    "    dst = np.sqrt(np.sum(df**2))\n",
    "    if dst>0:\n",
    "        print(dst)\n",
    "    loss.append(dst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate GM similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cos_distance(gram1,gram2): \n",
    "    cos = nn.CosineSimilarity(dim=0, eps=1e-6)\n",
    "    distance = np.zeros((6,1)) \n",
    "    for i in range(6): \n",
    "        temp= cos(gram1[i], gram2[i]) \n",
    "        distance[i]=temp\n",
    "    if distance.mean()<1:\n",
    "        print(1-distance.mean())\n",
    "    return distance.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.313225746154785e-05\n",
      "4.470348358154297e-07\n",
      "8.589029312133789e-05\n",
      "0.00016096234321594238\n",
      "0.00015812118848168577\n",
      "0.0001634657382965088\n",
      "8.894006411230837e-05\n",
      "9.05295213063928e-05\n",
      "0.00016117095947265625\n"
     ]
    }
   ],
   "source": [
    "numStreams=6\n",
    "testfolder= 'inputs/new_texture/FM_cf'\n",
    "import os\n",
    "input_imgs = []\n",
    "listdir = os.listdir(testfolder)\n",
    "#print(listdir)\n",
    "for filename in listdir:\n",
    "    if 'exp-00.00'in filename:\n",
    "        if '.wav'in filename:\n",
    "            input_imgs.append(testfolder+'/'+filename)\n",
    "    \n",
    "\n",
    "input_imgs.sort()\n",
    "#print(\"input_imgs:\",input_imgs)\n",
    "\n",
    "loss=np.zeros((26,1))\n",
    "gram_list=[] \n",
    "\n",
    "for i in range(len(input_imgs)):\n",
    "    input_img=prepare_input(input_imgs[i])\n",
    "    result=[]\n",
    "    for j in range(numStreams):\n",
    "        gram1=get_gram_v2(cnnlist[j],result,input_img, None, 1, 0)\n",
    "    gram_list.append(result)\n",
    "\n",
    "for i in range(len(gram_list)):\n",
    "    loss[i]=compute_cos_distance(gram_list[7],gram_list[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate the GM Distance (Normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_mse_distance(gram1,gram2):\n",
    "    distance = np.zeros((6,1)) \n",
    "    for i in range(6): \n",
    "        temp= mse(gram1[i], gram2[i]) \n",
    "        distance[i]=temp \n",
    "    if distance.sum()>0:\n",
    "        print(distance.sum())\n",
    "    return distance.sum()/6 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00302348579862155\n",
      "0.0027858100074809045\n",
      "0.00041847946886264253\n",
      "2.039849709944974e-05\n",
      "0.0004904257511952892\n",
      "0.0009825406013987958\n",
      "0.0030228441319195554\n",
      "0.002776503926725127\n",
      "0.002386089725405327\n"
     ]
    }
   ],
   "source": [
    "numStreams=6\n",
    "testfolder= 'inputs/new_texture/FM_cf'\n",
    "import os\n",
    "input_imgs = []\n",
    "listdir = os.listdir(testfolder)\n",
    "#print(listdir)\n",
    "for filename in listdir:\n",
    "    if 'exp-00.00'in filename:\n",
    "        if '.wav'in filename:\n",
    "            input_imgs.append(testfolder+'/'+filename)\n",
    "            \n",
    "input_imgs.sort()\n",
    "#print(\"input_imgs:\",input_imgs)\n",
    "\n",
    "loss=np.zeros((20,1))\n",
    "gram_list=[] #size:20*6*512*5112\n",
    "\n",
    "for i in range(len(input_imgs)):\n",
    "    input_img=prepare_input(input_imgs[i])\n",
    "    result=[]\n",
    "    for j in range(numStreams):\n",
    "        gram1=get_gram(cnnlist[j],result,input_img, None, 1, 0)\n",
    "    gram_list.append(result)\n",
    "\n",
    "\n",
    "for i in range(len(gram_list)):\n",
    "    loss[i]= compute_mse_distance(gram_list[9],gram_list[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
