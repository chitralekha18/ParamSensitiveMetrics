{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "import librosa\n",
    "from IPython.display import Audio, display\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as stats\n",
    "\n",
    "from torch.nn.modules.module import _addindent\n",
    "\n",
    "import copy\n",
    "import os\n",
    "import math\n",
    "\n",
    "from scipy.stats import ortho_group\n",
    "\n",
    "import soundfile as sf\n",
    "from matplotlib.pyplot import figure\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics.pairwise import cosine_distances as cos\n",
    "from sklearn.metrics import mean_squared_error as mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_scale(img):\n",
    "    img = np.log1p(img)\n",
    "    return img\n",
    "\n",
    "def inv_log(img):\n",
    "    img = np.exp(img) - 1.\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_steps=1\n",
    "numStreams=6\n",
    "learning_Rate= 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "runs = 1 \n",
    "N_FFT = 512 \n",
    "K_HOP = 128 \n",
    "N_FREQ=257\n",
    "N_FILTERS = 512\n",
    "\n",
    "possible_kernels = [2,4,8,16,64,128,256,512,1024,2048]\n",
    "hor_filters = [0]*numStreams\n",
    "for j in range(numStreams):\n",
    "    hor_filters[j]=possible_kernels[j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_spectrogram=log_scale\n",
    "inv_log_spectrogram=inv_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU available = False\n"
     ]
    }
   ],
   "source": [
    "use_cuda = torch.cuda.is_available() #use GPU if available\n",
    "print('GPU available =',use_cuda)\n",
    "dtype = torch.cuda.FloatTensor if use_cuda else torch.FloatTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_audio_spectum(filename):\n",
    "    x, fs  = sf.read(filename)\n",
    "    N_SAMPLES = len(x)   \n",
    "    R = np.abs(librosa.stft(x, n_fft=N_FFT, hop_length=K_HOP, win_length=N_FFT,  center=False))    \n",
    "    return R,fs\n",
    "\n",
    "\n",
    "def findMinMax(img):\n",
    "    return int(math.floor(np.amin(img))),int(math.ceil(np.amax(img)))\n",
    "\n",
    "def img_scale(img,datasetMin,datasetMax,scaleMin,scaleMax):\n",
    "    \"\"\"scales input numpy array from [datasetMin,datasetMax] -> [scaleMin,scaleMax]\"\"\"    \n",
    "    shift = (scaleMax-scaleMin) / (datasetMax-datasetMin)\n",
    "    scaled_values = shift * (img-datasetMin) + scaleMin\n",
    "    return scaled_values\n",
    "\n",
    "def img_invscale(img,datasetMin,datasetMax,scaleMin,scaleMax):\n",
    "    \"\"\"scales input numpy array from [scaleMin,scaleMax] -> [datasetMin,datasetMax]\"\"\"\n",
    "    shift = (datasetMax-datasetMin) / (scaleMax-scaleMin)\n",
    "    scaled_values = shift * (img-scaleMin) + datasetMin\n",
    "    return scaled_values\n",
    "    \n",
    "\n",
    "    def db_scale(img,scale=80):\n",
    "        img = librosa.amplitude_to_db(img)\n",
    "        shift = float(np.amax(img))\n",
    "        img = img - shift \n",
    "        img = img/scale \n",
    "        img = img + 1\n",
    "        img = np.maximum(img, 0)\n",
    "        return img, shift\n",
    "\n",
    "    def inv_db(img,shift,scale=80):\n",
    "        img = img - 1 \n",
    "        img = img * scale \n",
    "        img = img + shift\n",
    "        img = librosa.db_to_amplitude(img)    \n",
    "        return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_input(FILENAME):\n",
    "    R, fs = read_audio_spectum(FILENAME)   \n",
    "    a_style = log_scale(R)\n",
    "    a_style = np.ascontiguousarray(a_style[None,None,:,:])\n",
    "    a_style = torch.from_numpy(a_style).permute(0,2,1,3) \n",
    "    converted_img = Variable(a_style).type(dtype)\n",
    "    return converted_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"Here we create the custom network\"\n",
    "import collections as c\n",
    "\n",
    "IN_CHANNELS = N_FREQ\n",
    "def weights_init(m,hor_filter):\n",
    "    std = np.sqrt(2) * np.sqrt(2.0 / ((N_FREQ + N_FILTERS) * hor_filter))\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        m.weight.data.normal_(0.0, std)\n",
    "\n",
    "class style_net(nn.Module):\n",
    "    \"\"\"Here create the network you want to use by adding/removing layers in nn.Sequential\"\"\"\n",
    "    def __init__(self,hor_filter):\n",
    "        super(style_net, self).__init__()\n",
    "        self.layers = nn.Sequential(c.OrderedDict([\n",
    "                            ('conv1',nn.Conv2d(IN_CHANNELS,N_FILTERS,kernel_size=(1,hor_filter),bias=False)),\n",
    "                            ('relu1',nn.ReLU())]))\n",
    "\n",
    "            \n",
    "    def forward(self,input):\n",
    "        out = self.layers(input)\n",
    "        return out\n",
    "    \n",
    "cnnlist=[] \n",
    "for j in range(numStreams) :\n",
    "    cnn = style_net(hor_filters[j])\n",
    "    cnn.apply(lambda x, f=hor_filters[j]: weights_init(x,f))\n",
    "    for param in cnn.parameters():\n",
    "        param.requires_grad = False\n",
    "    if use_cuda:\n",
    "        cnn = cnn.cuda()\n",
    "    \n",
    "    cnnlist.append(cnn)\n",
    "\n",
    "\n",
    "content_layers_default = [] \n",
    "style_layers_default = ['relu_1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GramMatrix(nn.Module):\n",
    "\n",
    "    def forward(self, input):\n",
    "        a, b, c, d = input.size()         \n",
    "        features = input.view(b, a * c * d)\n",
    "        features2=features.unsqueeze(0)\n",
    "        G = torch.matmul(features2, torch.transpose(features2, 1,2))\n",
    "        return G.div(a * c * d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#return normalized gram matrix\n",
    "def get_gram(cnn,result,style_img, content_img=None,\n",
    "                               style_weight=1, content_weight=0,\n",
    "                               content_layers=content_layers_default,\n",
    "                               style_layers=style_layers_default, style_img2=None):\n",
    "    cnn = copy.deepcopy(cnn)\n",
    "    \n",
    "    model = nn.Sequential()\n",
    "    layer_list = list(cnn.layers)\n",
    "    \n",
    "    gram = GramMatrix()\n",
    "     \n",
    "    i = 1  \n",
    "    for layer in layer_list:\n",
    "        \n",
    "        if isinstance(layer, nn.Conv2d): \n",
    "            name = \"conv_\" + str(i)\n",
    "            model.add_module(name, layer) \n",
    "\n",
    "            if name in style_layers: \n",
    "                target_feature = model(style_img).clone()\n",
    "                target_feature_gram = gram(target_feature)\n",
    "                target_feature_gram=torch.flatten(target_feature_gram)\n",
    "                target_feature_gram=target_feature_gram.numpy()\n",
    "                target_feature_gram=target_feature_gram.reshape(512,512)\n",
    "                target_feature_gram= StandardScaler().fit_transform(target_feature_gram)\n",
    "                result.append((target_feature_gram))\n",
    "\n",
    "        if isinstance(layer, nn.ReLU):\n",
    "            name = \"relu_\" + str(i)\n",
    "            model.add_module(name, layer)\n",
    "                    \n",
    "            if name in style_layers:\n",
    "                target_feature = model(style_img).clone()\n",
    "                target_feature_gram = gram(target_feature)\n",
    "                target_feature_gram=torch.flatten(target_feature_gram)\n",
    "                target_feature_gram=target_feature_gram.numpy()\n",
    "                target_feature_gram=target_feature_gram.reshape(512,512)\n",
    "                target_feature_gram= StandardScaler().fit_transform(target_feature_gram)\n",
    "                result.append((target_feature_gram))\n",
    "               \n",
    "            i += 1\n",
    "\n",
    "        if isinstance(layer, nn.MaxPool2d): \n",
    "            name = \"pool_\" + str(i)\n",
    "            model.add_module(name, layer)\n",
    "            \n",
    "            if name in style_layers:\n",
    "                target_feature = model(style_img).clone()\n",
    "                target_feature_gram = gram(target_feature)\n",
    "                target_feature_gram=torch.flatten(target_feature_gram)\n",
    "                target_feature_gram=target_feature_gram.numpy()\n",
    "                target_feature_gram=target_feature_gram.reshape(512,512)\n",
    "                target_feature_gram= StandardScaler().fit_transform(target_feature_gram)\n",
    "                result.append((target_feature_gram))\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gram_v2(cnn,result,style_img, content_img=None,\n",
    "                               style_weight=1, content_weight=0,\n",
    "                               content_layers=content_layers_default,\n",
    "                               style_layers=style_layers_default, style_img2=None):\n",
    "    cnn = copy.deepcopy(cnn)\n",
    "    \n",
    "    model = nn.Sequential()\n",
    "    layer_list = list(cnn.layers)\n",
    "    \n",
    "    gram = GramMatrix()  \n",
    "    i = 1  \n",
    "    for layer in layer_list:\n",
    "        \n",
    "        if isinstance(layer, nn.Conv2d):\n",
    "            name = \"conv_\" + str(i)\n",
    "            model.add_module(name, layer) \n",
    "\n",
    "            if name in style_layers:\n",
    "                target_feature = model(style_img).clone()\n",
    "                target_feature_gram = gram(target_feature)\n",
    "                target_feature_gram=torch.flatten(target_feature_gram)\n",
    "                result.append((target_feature_gram))\n",
    "\n",
    "        if isinstance(layer, nn.ReLU):\n",
    "            name = \"relu_\" + str(i)\n",
    "            model.add_module(name, layer)\n",
    "                    \n",
    "            if name in style_layers:\n",
    "                target_feature = model(style_img).clone()\n",
    "                target_feature_gram = gram(target_feature)\n",
    "                target_feature_gram=torch.flatten(target_feature_gram)\n",
    "                result.append((target_feature_gram))\n",
    "               \n",
    "            i += 1\n",
    "\n",
    "        if isinstance(layer, nn.MaxPool2d): #do the same for maxpool\n",
    "            name = \"pool_\" + str(i)\n",
    "            model.add_module(name, layer)\n",
    "            \n",
    "            if name in style_layers:\n",
    "                target_feature = model(style_img).clone()\n",
    "                target_feature_gram = gram(target_feature)\n",
    "                target_feature_gram=torch.flatten(target_feature_gram)\n",
    "                result.append((target_feature_gram))\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate L2 Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean_distance(img_1, img_2):\n",
    "    df = np.asarray(img_1-img_2)\n",
    "    dst = np.sqrt(np.sum(df**2))\n",
    "    return dst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate GM Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cos_distance(gram1,gram2): \n",
    "    cos = nn.CosineSimilarity(dim=0, eps=1e-6)\n",
    "    distance = np.zeros((6,1))\n",
    "    for i in range(6): \n",
    "        temp= cos(gram1[i], gram2[i]) \n",
    "        distance[i]=temp\n",
    "    return 1-distance.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate GM Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_mse_distance(gram1,gram2):\n",
    "    distance = np.zeros((6,1))\n",
    "    for i in range(6): \n",
    "        temp= mse(gram1[i], gram2[i]) \n",
    "        distance[i]=temp \n",
    "    return distance.sum() "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_agm",
   "language": "python",
   "name": "venv_agm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
